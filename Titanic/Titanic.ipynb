{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.5.0-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy in /Users/kosmos/opt/anaconda3/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in /Users/kosmos/opt/anaconda3/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import math\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Transformers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# K-Fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "submission_data = pd.read_csv('./data/gender_submission.csv')\n",
    "\n",
    "\n",
    "X = train.drop(\"Survived\",axis=1)\n",
    "y = train[\"Survived\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificatoin stratagy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e30c31e132a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic_df' is not defined"
     ]
    }
   ],
   "source": [
    "titanic_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class AgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        self.mean = X['Age'].mean() \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Age'] = X_['Age'].fillna((self.mean))\n",
    "        return X_\n",
    "\n",
    "# Pipline for Age\n",
    "age_clean_pipeline = Pipeline([\n",
    "    ('immuter', AgeTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class CabinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Cabin'] = X_['Cabin'].fillna('N')\n",
    "        return X_\n",
    "    \n",
    "# Pipline for Cabin\n",
    "cabin_clean_pipeline = Pipeline([\n",
    "    ('immuter', CabinTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FareTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        self.mean = X['Fare'].mean() \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Fare'] = X_['Fare'].fillna((self.mean))\n",
    "        return X_\n",
    "\n",
    "# Pipline for Fare\n",
    "fare_clean_pipeline = Pipeline([\n",
    "    ('imputer', FareTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class EmbarkedTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Embarked'] = X_['Embarked'].fillna('N')\n",
    "        return X_\n",
    "    \n",
    "# Pipline for Embarked\n",
    "embarked_clean_pipeline = Pipeline([\n",
    "    ('immuter', EmbarkedTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hase cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class HasCabinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Has_Cabin'] = X_['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
    "        return X_\n",
    "    \n",
    "add_has_cabin_pipline = Pipeline([\n",
    "    ('new', HasCabinTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cabin_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class CabinLabelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Cabin_Label'] = X_['Cabin'].str.get(0)\n",
    "        return X_\n",
    "    \n",
    "add_cabin_label_pipline = Pipeline([\n",
    "    ('new', CabinLabelTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FamilySizeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Family_Size'] = X_['SibSp'] + X_['Parch'] + 1\n",
    "        return X_\n",
    "    \n",
    "add_family_size_pipline = Pipeline([\n",
    "    ('new', FamilySizeTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class IsAloneTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Is_Alone'] = 0\n",
    "        X_.loc[X_['Family_Size'] == 1, 'Is_Alone'] = 1\n",
    "        return X_\n",
    "    \n",
    "add_is_alone_pipline = Pipeline([\n",
    "    ('new', IsAloneTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "class TitleTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_ = X.copy()\n",
    "        X_['Title'] = X_['Name'].apply(get_title)\n",
    "        X_['Title'] = X_['Title'].replace('Mlle', 'Miss')\n",
    "        X_['Title'] = X_['Title'].replace('Ms', 'Miss')\n",
    "        X_['Title'] = X_['Title'].replace('Mme', 'Mrs')\n",
    "        return X_\n",
    "    \n",
    "add_title_pipline = Pipeline([\n",
    "    ('new', TitleTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_before_clean_pipline = Pipeline([\n",
    "    ('Has_Cabin', add_has_cabin_pipline),\n",
    "])\n",
    "\n",
    "clean_pipeline = Pipeline([\n",
    "    ('Age', age_clean_pipeline),\n",
    "    ('Cabin', cabin_clean_pipeline),\n",
    "    ('Embarked', embarked_clean_pipeline),\n",
    "    ('Fare', fare_clean_pipeline),\n",
    "])\n",
    "\n",
    "add_after_clean_pipline = Pipeline([\n",
    "    ('Cabin_Label', add_cabin_label_pipline),\n",
    "    ('Family_Size', add_family_size_pipline),\n",
    "    ('Is_Alone', add_is_alone_pipline),\n",
    "    ('Title', add_title_pipline),\n",
    "])\n",
    "\n",
    "cat_pipline = Pipeline([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "num_pipline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "categoties = ['Has_Cabin', 'Sex', 'Cabin', 'Embarked', 'Pclass', 'Cabin_Label', 'Is_Alone', 'Title']\n",
    "num = ['Age', 'Fare', 'SibSp', 'Parch', 'Family_Size']\n",
    "\n",
    "feats = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipline, num),\n",
    "    ('cat', cat_pipline, categoties),\n",
    "])\n",
    "\n",
    "feature_processing = Pipeline([\n",
    "    ('add before', add_before_clean_pipline),\n",
    "    ('clean', clean_pipeline),\n",
    "    ('add after', add_after_clean_pipline),\n",
    "    ('feats', feats),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Best estimator: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=42, splitter='best')\n",
      "--------------------------------------------------------\n",
      "Best score: 0.8260381593714928\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Decision tree model\n",
    "model=DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "    'min_samples_leaf': [1,2,3,4,5,10,20]\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(model, param_grid, cv=skf, scoring=\"accuracy\", return_train_score=True)\n",
    "\n",
    "# Transform data\n",
    "X_t = feature_processing.fit_transform(X)\n",
    "\n",
    "\n",
    "# Fit model\n",
    "gscv.fit(X_t, y)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best estimator:\", gscv.best_estimator_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best score:\", gscv.best_score_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "# print('Mean test score: {}'.format(gscv.cv_results_['mean_test_score']))\n",
    "# print('Mean train score: {}'.format(gscv.cv_results_['mean_train_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base score: \n",
    "\n",
    "v1 = 0.8148148148148149\n",
    "v3 = 0.8237934904601572\n",
    "v4 = 0.8159371492704827\n",
    "v5 = 0.8204264870931538\n",
    "v6 = 0.8260381593714928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First submit to kagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-408-ce4fec58ff22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlast_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msubmission_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msubmission_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./submissions/submission_decision_tree.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3636\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "X_test_t = feature_processing.transform(X_test)\n",
    "\n",
    "last_prediction = gscv.best_estimator_.predict(X_test_t)\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_decision_tree.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result on test data on Kagle: \n",
    "v1 = 0.78708\n",
    "v2 = 0.73684\n",
    "v3 = 0.76315\n",
    "v6 = 0.76555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging using sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Best estimator: DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=42, splitter='best')\n",
      "--------------------------------------------------------\n",
      "Best score: 0.8260381593714928\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : [50, 100, 150, 200, 300, 400, 500, 1000, 1500, 2000, 2500, 3000, 3500],\n",
    "    'max_samples' : [100, 200, 250, 300, 350, 400, 450, 500, 550]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "    BaggingClassifier(\n",
    "        DecisionTreeClassifier(\n",
    "            random_state=42),\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "model.fit(X_t, y)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best estimator:\", gscv.best_estimator_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best score:\", gscv.best_score_)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First submit to kagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = feature_processing.transform(X_test)\n",
    "last_prediction = model.best_estimator_.predict(X_test_t)\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_bagging_sklearn.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finul score on Kagel: 0.77751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Best estimator: BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                        class_weight=None,\n",
      "                                                        criterion='gini',\n",
      "                                                        max_depth=None,\n",
      "                                                        max_features=None,\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=1,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        presort='deprecated',\n",
      "                                                        random_state=42,\n",
      "                                                        splitter='best'),\n",
      "                  bootstrap=True, bootstrap_features=False, max_features=0.71,\n",
      "                  max_samples=0.7, n_estimators=100, n_jobs=-1, oob_score=True,\n",
      "                  random_state=42, verbose=0, warm_start=False)\n",
      "--------------------------------------------------------\n",
      "Best score: 0.8417425145941874\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : [10, 20, 50, 100, 150, 200, 300, 400, 500, 1000, 2000, 3000],\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.3, 0.35, 0.4, 0.45, 0.5, 0.7, 0.9],\n",
    "    'max_features' : [0.6, 0.65, 0.7, 0.71, 0.72, 0.73, 0.8]\n",
    "}\n",
    "\n",
    "\n",
    "model = GridSearchCV(BaggingClassifier(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    param_grid,         \n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "model.fit(X_t, y)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best estimator:\", model.best_estimator_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best score:\", model.best_score_)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Best estimator: BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                        class_weight=None,\n",
      "                                                        criterion='gini',\n",
      "                                                        max_depth=6,\n",
      "                                                        max_features=None,\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=2,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        presort='deprecated',\n",
      "                                                        random_state=42,\n",
      "                                                        splitter='best'),\n",
      "                  bootstrap=True, bootstrap_features=False, max_features=0.72,\n",
      "                  max_samples=0.75, n_estimators=90, n_jobs=-1, oob_score=True,\n",
      "                  random_state=42, verbose=0, warm_start=False)\n",
      "--------------------------------------------------------\n",
      "Best score: 0.8394953235829515\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : [90, 100, 120],\n",
    "    'max_samples' : [0.6, 0.65, 0.69, 0.7, 0.71, 0.75, 0.8],\n",
    "    'max_features' : [0.7, 0.71, 0.72, 0.73],\n",
    "    'base_estimator__max_depth': [3, 4, 5, 6, 7],\n",
    "    'base_estimator__min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "\n",
    "model = GridSearchCV(BaggingClassifier(\n",
    "        DecisionTreeClassifier(\n",
    "            random_state=42,\n",
    "            criterion='gini',\n",
    "            max_depth=4,\n",
    "            min_samples_leaf=1,\n",
    "        ),\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    param_grid,         \n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "model.fit(X_t, y)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best estimator:\", model.best_estimator_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best score:\", model.best_score_)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = feature_processing.transform(X_test)\n",
    "last_prediction = model.predict(X_test_t)\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_bagging_sklearn.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kegle: 0.77990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging with Custom Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBaggingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    estimators = []\n",
    "    \n",
    "    def __init__(self, base_estimator, n_estimators=10, max_samples=1.0, max_features=1.0, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __filter(self, X, y, features, samples):\n",
    "        \"\"\" Helper which filter samples and features by list of indexes \"\"\"\n",
    "        new_X = []\n",
    "        new_y = []\n",
    "        \n",
    "        X_filtered = np.take(X.toarray(), features, axis=1)\n",
    "        \n",
    "        for i in samples:\n",
    "            new_X.append(X_filtered[i])\n",
    "            new_y.append(y[i])\n",
    "        \n",
    "        new_X = np.array(new_X)\n",
    "        new_y = np.array(new_y)\n",
    "        \n",
    "        return (new_X, new_y)\n",
    "    \n",
    "    \n",
    "    def __filter_only_features(self, X, features):\n",
    "        \"\"\" Helper which filter features by list of indexes \"\"\"\n",
    "        new_X = []\n",
    "        \n",
    "        X_filtered = np.take(X.toarray(), features, axis=1)\n",
    "        \n",
    "        return X_filtered\n",
    "  \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        estimators = []\n",
    "        \n",
    "        number_of_features = X.shape[1]\n",
    "        number_of_samples = X.shape[0]\n",
    "        \n",
    "        number_of_features_to_use = math.floor(number_of_features * self.max_features)\n",
    "        number_of_samples_to_use = math.floor(number_of_samples * self.max_samples)\n",
    "        \n",
    "        all_features = range(0, number_of_features)\n",
    "        all_samples = range(0, number_of_samples)\n",
    "        \n",
    "        for i in range(0, self.n_estimators):\n",
    "            features = random.sample(all_features, number_of_features_to_use)\n",
    "            samples = random.choices(all_samples, k=number_of_samples_to_use)\n",
    "            features.sort()\n",
    "            \n",
    "            (new_X, new_y) = self.__filter(X, y, features, samples)\n",
    "            \n",
    "            estimators.append({\n",
    "                'features': features,\n",
    "                'samples': samples,\n",
    "                'estimator': self.base_estimator.fit(new_X, new_y),\n",
    "            })\n",
    "            \n",
    "        self.estimators = estimators\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            X_with_droped_features = self.__filter_only_features(X, estimator['features'])\n",
    "            preds.append(estimator['estimator'].predict(X_with_droped_features))\n",
    "        \n",
    "        return pd.DataFrame(preds).mode().transpose()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "X_t = feature_processing.fit_transform(X)\n",
    "\n",
    "# Model\n",
    "model = CustomBaggingClassifier(base_estimator=DecisionTreeClassifier(\n",
    "            random_state=42,\n",
    "            criterion='gini',\n",
    "            max_depth=4,\n",
    "            min_samples_leaf=1,\n",
    "        ),\n",
    "        n_estimators=90,\n",
    "        max_samples=0.75,\n",
    "        max_features=0.72,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     . :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 335\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 281\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "KeyError: 51\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "# #    \n",
    "results = cross_val_score(model, X_t, y, cv=skf)\n",
    "\n",
    "# #     \n",
    "print(f\"CV accuracy score: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_t, y)\n",
    "X_test_t = feature_processing.transform(X_test)\n",
    "last_prediction = model.predict(X_test_t)\n",
    "last_prediction\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_bagging_custom.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0.46172      :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 896 candidates, totalling 2688 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2688 out of 2688 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Best estimator: RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=10, max_features=20,\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "--------------------------------------------------------\n",
      "Best score: 0.8417508417508417\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [100, 200, 201, 202, 203, 204, 205, 1000],\n",
    "    'max_features': [4, 7, 10, 13, 14, 15, 20], \n",
    "    'min_samples_leaf': [2, 3, 5, 7], \n",
    "    'max_depth': [5,10,15,20],\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1) \n",
    "model = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "model.fit(X_t, y)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best estimator:\", model.best_estimator_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best score:\", model.best_score_)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = feature_processing.transform(X_test)\n",
    "last_prediction = model.predict(X_test_t)\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_random_forest.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kagle: 0.78229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 896 candidates, totalling 2688 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2688 out of 2688 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Best estimator: ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=10, max_features=20,\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=3, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "                     oob_score=False, random_state=42, verbose=0,\n",
      "                     warm_start=False)\n",
      "--------------------------------------------------------\n",
      "Best score: 0.8316498316498316\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [100, 200, 201, 202, 203, 204, 205, 1000],\n",
    "    'max_features': [4, 7, 10, 13, 14, 15, 20], \n",
    "    'min_samples_leaf': [2, 3, 5, 7], \n",
    "    'max_depth': [5,10,15,20],\n",
    "}\n",
    "\n",
    "etc = ExtraTreesClassifier(random_state=42, n_jobs=-1) \n",
    "model = GridSearchCV(etc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "model.fit(X_t, y)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best estimator:\", model.best_estimator_)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Best score:\", model.best_score_)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = feature_processing.transform(X_test)\n",
    "last_prediction = model.predict(X_test_t)\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_extra_tree.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle: 0.78947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.90263+0.00594\ttest-auc:0.86225+0.01217\n",
      "[10]\ttrain-auc:0.93743+0.00270\ttest-auc:0.87136+0.00403\n",
      "[20]\ttrain-auc:0.95242+0.00242\ttest-auc:0.87927+0.00480\n",
      "[30]\ttrain-auc:0.96296+0.00405\ttest-auc:0.88059+0.00737\n",
      "[40]\ttrain-auc:0.96916+0.00215\ttest-auc:0.88217+0.00929\n",
      "[50]\ttrain-auc:0.97430+0.00225\ttest-auc:0.88163+0.00956\n",
      "[60]\ttrain-auc:0.97987+0.00314\ttest-auc:0.88249+0.00809\n",
      "[70]\ttrain-auc:0.98292+0.00267\ttest-auc:0.88219+0.00707\n",
      "[80]\ttrain-auc:0.98576+0.00253\ttest-auc:0.88013+0.00801\n",
      "[90]\ttrain-auc:0.98786+0.00223\ttest-auc:0.87945+0.00827\n",
      "[99]\ttrain-auc:0.98974+0.00172\ttest-auc:0.87990+0.00908\n"
     ]
    }
   ],
   "source": [
    "# Transform data\n",
    "X_t = feature_processing.fit_transform(X)\n",
    "\n",
    "parameters = {\n",
    "    #default\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.1,\n",
    "    \"verbosity\": 0,\n",
    "    \"nthread\": 4,\n",
    "    \"random_seed\": 1,\n",
    "    \"eval_metric\": \"auc\"\n",
    "}\n",
    "\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_t, y)\n",
    "\n",
    "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
    "                 folds=skf, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.87799+0.00466\ttest-auc:0.85208+0.01454\n",
      "[10]\ttrain-auc:0.91819+0.00959\ttest-auc:0.87631+0.01530\n",
      "[20]\ttrain-auc:0.93048+0.00548\ttest-auc:0.88023+0.01787\n",
      "[30]\ttrain-auc:0.94078+0.00352\ttest-auc:0.87892+0.01645\n",
      "[40]\ttrain-auc:0.94887+0.00462\ttest-auc:0.87972+0.01822\n",
      "[50]\ttrain-auc:0.95443+0.00438\ttest-auc:0.87858+0.01650\n",
      "[60]\ttrain-auc:0.96124+0.00294\ttest-auc:0.87949+0.01828\n",
      "[70]\ttrain-auc:0.96570+0.00354\ttest-auc:0.88092+0.01714\n",
      "[80]\ttrain-auc:0.97015+0.00353\ttest-auc:0.87838+0.01637\n",
      "[90]\ttrain-auc:0.97454+0.00329\ttest-auc:0.87760+0.01485\n",
      "[99]\ttrain-auc:0.97754+0.00241\ttest-auc:0.87947+0.01548\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    #default\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.1,\n",
    "    \"verbosity\": 0,\n",
    "    \"nthread\": 4,\n",
    "    \"random_seed\": 1,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \n",
    "    # regularization parameters\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7\n",
    "}\n",
    "\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_t, y)\n",
    "\n",
    "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
    "                 folds=skf, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.88136+0.00727\ttest-auc:0.86125+0.00661\n",
      "[10]\ttrain-auc:0.92087+0.00658\ttest-auc:0.87688+0.01655\n",
      "[20]\ttrain-auc:0.93653+0.00504\ttest-auc:0.87954+0.01816\n",
      "[30]\ttrain-auc:0.94617+0.00414\ttest-auc:0.87993+0.01936\n",
      "[40]\ttrain-auc:0.95288+0.00563\ttest-auc:0.88136+0.01899\n",
      "[50]\ttrain-auc:0.96094+0.00447\ttest-auc:0.88262+0.01853\n",
      "[60]\ttrain-auc:0.96481+0.00527\ttest-auc:0.88234+0.01781\n",
      "[70]\ttrain-auc:0.96804+0.00502\ttest-auc:0.88195+0.01741\n",
      "[80]\ttrain-auc:0.97188+0.00446\ttest-auc:0.88088+0.01495\n",
      "[90]\ttrain-auc:0.97619+0.00462\ttest-auc:0.88208+0.01391\n",
      "[99]\ttrain-auc:0.97810+0.00413\ttest-auc:0.88076+0.01374\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    #default\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.1,\n",
    "    \"verbosity\": 0,\n",
    "    \"nthread\": 4,\n",
    "    \"random_seed\": 1,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \n",
    "    # regularization parameters\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \n",
    "    #lightgbm approach\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_t, y)\n",
    "\n",
    "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
    "                 folds=skf, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.88136+0.00727\ttest-auc:0.86125+0.00661\n",
      "[10]\ttrain-auc:0.92087+0.00658\ttest-auc:0.87688+0.01655\n",
      "[20]\ttrain-auc:0.93653+0.00504\ttest-auc:0.87954+0.01816\n",
      "[30]\ttrain-auc:0.94617+0.00414\ttest-auc:0.87993+0.01936\n",
      "[40]\ttrain-auc:0.95288+0.00563\ttest-auc:0.88136+0.01899\n",
      "[50]\ttrain-auc:0.96094+0.00447\ttest-auc:0.88262+0.01853\n",
      "[60]\ttrain-auc:0.96481+0.00527\ttest-auc:0.88234+0.01781\n",
      "[70]\ttrain-auc:0.96804+0.00502\ttest-auc:0.88195+0.01741\n",
      "[80]\ttrain-auc:0.97188+0.00446\ttest-auc:0.88088+0.01495\n",
      "[90]\ttrain-auc:0.97619+0.00462\ttest-auc:0.88208+0.01391\n",
      "[99]\ttrain-auc:0.97810+0.00413\ttest-auc:0.88076+0.01374\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    #default\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.1,\n",
    "    \"verbosity\": 0,\n",
    "    \"nthread\": 4,\n",
    "    \"random_seed\": 1,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \n",
    "    # regularization parameters\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \n",
    "    #lightgbm approach\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"grow_policy\": \"lossguide\"\n",
    "}\n",
    "\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_t, y)\n",
    "\n",
    "results = xgb.cv(parameters, xgb_train, num_boost_round=100,\n",
    "                 folds=skf, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosmos/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.7,\n",
       "              enable_categorical=False, eta=0.1, eval_metric='auc', gamma=0,\n",
       "              gpu_id=-1, grow_policy='lossguide', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.100000001,\n",
       "              max_delta_step=0, max_depth=5, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=4, nthread=4,\n",
       "              num_parallel_tree=1, objective='binary:logistic',\n",
       "              predictor='auto', random_seed=1, random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**parameters)\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = feature_processing.transform(X_test)\n",
    "last_prediction = xgb_clf.predict(X_test_t)\n",
    "submission_data['Survived'] = last_prediction.astype(int)\n",
    "submission_data.to_csv('./submissions/submission_xgboost.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "titanic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
